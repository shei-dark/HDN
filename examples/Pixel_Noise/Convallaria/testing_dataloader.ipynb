{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('/home/sheida.rahnamai/GIT/HDN/')\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lib.dataloader import CustomDataset, CombinedBatchSampler\n",
    "import tifffile as tiff\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from boilerplate import boilerplate\n",
    "import random\n",
    "from skimage.measure import regionprops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "patch_size = 64\n",
    "sample_size = 350\n",
    "centre_size = 4\n",
    "n_channel = 32\n",
    "hierarchy_level = 3\n",
    "pad_size = (patch_size - centre_size) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['uncategorized', 'nucleus', 'granule', 'mitochondria']\n",
    "train_labeled_indices = []\n",
    "val_labeled_indices = []\n",
    "for cls in classes:\n",
    "    with open(f'/group/jug/Sheida/pancreatic beta cells/download/train/10_percent_{cls}.pickle', 'rb') as file:\n",
    "        train_labeled_indices.extend(pickle.load(file))\n",
    "    with open(f'/group/jug/Sheida/pancreatic beta cells/download/val/10_percent_{cls}.pickle', 'rb') as file:\n",
    "        val_labeled_indices.extend(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381 1177\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labeled_indices), len(val_labeled_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/group/jug/Sheida/pancreatic beta cells/download/train/1_percent_mitochondria.pickle'\n",
    "dir_save = '/group/jug/Sheida/pancreatic beta cells/download/train/0.1_percent_mitochondria.pickle'\n",
    "with open(dir, 'rb') as file:\n",
    "        x = pickle.load(file)\n",
    "y = random.sample(x, len(x)//10)\n",
    "with open(dir_save, 'wb') as file:\n",
    "    pickle.dump(y, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "dir = '/group/jug/Sheida/pancreatic beta cells/download/train/1_percent_mitochondria.pickle'\n",
    "with open(dir, 'rb') as file:\n",
    "        x = pickle.load(file)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering out outside of the cell: 100%|██████████| 3/3 [00:00<00:00,  4.51it/s]\n",
      "Normalizing data: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "\n",
    "data_dir = \"/group/jug/Sheida/pancreatic beta cells/download/\"\n",
    "keys = ['high_c1', 'high_c2', 'high_c3']\n",
    "\n",
    "# Load source images\n",
    "train_img_paths = [os.path.join(data_dir + 'train/' + key + f\"/{key}_source.tif\") for key in keys]\n",
    "train_lbl_paths = [os.path.join(data_dir + 'train/' + key + f\"/{key}_gt.tif\") for key in keys]\n",
    "val_img_paths = [os.path.join(data_dir + 'val/' + key + f\"/{key}_source.tif\") for key in keys]\n",
    "val_lbl_paths = [os.path.join(data_dir + 'val/' + key + f\"/{key}_gt.tif\") for key in keys]\n",
    "\n",
    "train_images = {key: tiff.imread(path) for key, path in zip(keys, train_img_paths)}\n",
    "train_labels = {key: tiff.imread(path) for key, path in zip(keys, train_lbl_paths)}\n",
    "\n",
    "val_images = {key: tiff.imread(path) for key, path in zip(keys, val_img_paths)}\n",
    "val_labels = {key: tiff.imread(path) for key, path in zip(keys, val_lbl_paths)}\n",
    "\n",
    "for key in tqdm(keys, desc='filtering out outside of the cell'):\n",
    "   filtered_image, filtered_label = boilerplate._filter_slices(train_images[key], train_labels[key])\n",
    "   train_images[key] = filtered_image\n",
    "   train_labels[key] = filtered_label\n",
    "\n",
    "   filtered_image, filtered_label = boilerplate._filter_slices(val_images[key], val_labels[key])\n",
    "   \n",
    "   val_images[key] = filtered_image\n",
    "   val_labels[key] = filtered_label\n",
    "\n",
    "# compute mean and std of the data\n",
    "all_elements = np.concatenate([train_images[key].flatten() for key in keys])\n",
    "data_mean = np.mean(all_elements)\n",
    "data_std = np.std(all_elements)\n",
    "\n",
    "# normalizing the data\n",
    "for key in tqdm(keys, 'Normalizing data'):\n",
    "   train_images[key] = (train_images[key] - data_mean) / data_std\n",
    "   val_images[key] = (val_images[key] - data_mean) / data_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'regionprops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m CustomDataset(train_images, train_labels)\n\u001b[1;32m      2\u001b[0m val_set \u001b[38;5;241m=\u001b[39m CustomDataset(val_images, val_labels)\n",
      "File \u001b[0;32m~/GIT/HDN/examples/Pixel_Noise/Convallaria/../../../lib/dataloader.py:94\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, images, labels, patch_size, mask_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_size \u001b[38;5;241m=\u001b[39m mask_size\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_patches \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# List to store all patches (with different labels)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatches_by_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_valid_patches(images, labels)\n",
      "File \u001b[0;32m~/GIT/HDN/examples/Pixel_Noise/Convallaria/../../../lib/dataloader.py:121\u001b[0m, in \u001b[0;36mCustomDataset._extract_valid_patches\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m    119\u001b[0m height, width \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Assuming 'mask' is the binary segmentation mask for the object\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m props \u001b[38;5;241m=\u001b[39m regionprops(lbl)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Get bounding box (min_row, min_col, max_row, max_col)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m props:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regionprops' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = CustomDataset(train_images, train_labels)\n",
    "val_set = CustomDataset(val_imvages, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 103821, val set size: 11781\n",
      "Number of given GT labels in train data: 10381, Number of given GT labels in val data: 1177\n"
     ]
    }
   ],
   "source": [
    "print(f\"train set size: {len(train_set)}, val set size: {len(val_set)}\")\n",
    "print(f\"Number of given GT labels in train data: {len(train_labeled_indices)}, Number of given GT labels in val data: {len(val_labeled_indices)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
