{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('/home/sheida.rahnamai/GIT/HDN/')\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lib.dataloader import CustomDataset, CombinedCustomDataset\n",
    "import tifffile as tiff\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from boilerplate import boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "patch_size = 64\n",
    "sample_size = 350\n",
    "centre_size = 4\n",
    "n_channel = 32\n",
    "hierarchy_level = 3\n",
    "pad_size = (patch_size - centre_size) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['uncategorized', 'nucleus', 'granule', 'mitochondria']\n",
    "train_labeled_indices = []\n",
    "val_labeled_indices = []\n",
    "for cls in classes:\n",
    "    with open(f'/group/jug/Sheida/pancreatic beta cells/download/train/10_percent_{cls}.pickle', 'rb') as file:\n",
    "        train_labeled_indices.extend(pickle.load(file))\n",
    "    with open(f'/group/jug/Sheida/pancreatic beta cells/download/val/10_percent_{cls}.pickle', 'rb') as file:\n",
    "        val_labeled_indices.extend(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering out outside of the cell: 100%|██████████| 3/3 [00:00<00:00,  4.65it/s]\n",
      "Normalizing data: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s]\n",
      "Extracting patches from high_c1: 965it [00:02, 340.90it/s]\n",
      "Extracting patches from high_c2: 889it [00:02, 338.36it/s]\n",
      "Extracting patches from high_c3: 907it [00:02, 350.74it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 365.69it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 403.80it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 348.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "\n",
    "data_dir = \"/group/jug/Sheida/pancreatic beta cells/download/\"\n",
    "keys = ['high_c1', 'high_c2', 'high_c3']\n",
    "\n",
    "# Load source images\n",
    "train_img_paths = [os.path.join(data_dir + 'train/' + key + f\"/{key}_source.tif\") for key in keys]\n",
    "train_lbl_paths = [os.path.join(data_dir + 'train/' + key + f\"/{key}_gt.tif\") for key in keys]\n",
    "val_img_paths = [os.path.join(data_dir + 'val/' + key + f\"/{key}_source.tif\") for key in keys]\n",
    "val_lbl_paths = [os.path.join(data_dir + 'val/' + key + f\"/{key}_gt.tif\") for key in keys]\n",
    "\n",
    "train_images = {key: tiff.imread(path) for key, path in zip(keys, train_img_paths)}\n",
    "train_labels = {key: tiff.imread(path) for key, path in zip(keys, train_lbl_paths)}\n",
    "\n",
    "val_images = {key: tiff.imread(path) for key, path in zip(keys, val_img_paths)}\n",
    "val_labels = {key: tiff.imread(path) for key, path in zip(keys, val_lbl_paths)}\n",
    "\n",
    "for key in tqdm(keys, desc='filtering out outside of the cell'):\n",
    "   filtered_image, filtered_label = boilerplate._filter_slices(train_images[key], train_labels[key])\n",
    "   train_images[key] = filtered_image\n",
    "   train_labels[key] = filtered_label\n",
    "\n",
    "   filtered_image, filtered_label = boilerplate._filter_slices(val_images[key], val_labels[key])\n",
    "   \n",
    "   val_images[key] = filtered_image\n",
    "   val_labels[key] = filtered_label\n",
    "\n",
    "# compute mean and std of the data\n",
    "all_elements = np.concatenate([train_images[key].flatten() for key in keys])\n",
    "data_mean = np.mean(all_elements)\n",
    "data_std = np.std(all_elements)\n",
    "\n",
    "# normalizing the data\n",
    "for key in tqdm(keys, 'Normalizing data'):\n",
    "   train_images[key] = (train_images[key] - data_mean) / data_std\n",
    "   val_images[key] = (val_images[key] - data_mean) / data_std\n",
    "\n",
    "train_set = CombinedCustomDataset(train_images, train_labels, train_labeled_indices)\n",
    "val_set = CombinedCustomDataset(val_images, val_labels, val_labeled_indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
