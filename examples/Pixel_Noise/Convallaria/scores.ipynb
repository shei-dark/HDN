{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('/home/sheida.rahnamai/GIT/HDN/')\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tifffile import imread\n",
    "import glob\n",
    "import matplotlib.patches as patches\n",
    "import hdbscan as hd\n",
    "from hdbscan import prediction\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from collections import Counter\n",
    "from openTSNE import TSNE\n",
    "from openTSNE import TSNEEmbedding\n",
    "from openTSNE import affinity\n",
    "from openTSNE import initialization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "# %reload lib.plotting \n",
    "from lib import plotting as p\n",
    "from lib import dataprep as dp\n",
    "import random\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "from lib.evaluation import FeatureExtractor\n",
    "# from lib.dataloader import CustomTestDataset\n",
    "from lib.dataloader import CustomDataset\n",
    "import tifffile as tiff\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from boilerplate import boilerplate\n",
    "from importlib import reload\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from lib.logging import ClassType\n",
    "from sklearn.metrics import confusion_matrix, adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score\n",
    "from boilerplate import boilerplate as blp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "patch_size = 64\n",
    "sample_size = 300\n",
    "centre_size = 4\n",
    "n_channel = 32\n",
    "hierarchy_level = 3\n",
    "pad_size = (patch_size - centre_size) // 2\n",
    "model_dir = \"/group/jug/Sheida/HVAE/v01/model/\"\n",
    "# model_dir = \"/home/sheida.rahnamai/GIT/HDN/examples/Pixel_Noise/Convallaria/testmodel/\"\n",
    "model = torch.load(model_dir+\"HVAE_best_vae.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images loaded from paths:\n",
      "/localscratch/data/high_c1/high_c1_source.tif\n",
      "/localscratch/data/high_c2/high_c2_source.tif\n",
      "/localscratch/data/high_c3/high_c3_source.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data: 100%|██████████| 3/3 [00:01<00:00,  2.84it/s]\n",
      "Normalizing data: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n",
      "Extracting patches from high_c1: 966it [00:03, 293.10it/s]\n",
      "Extracting patches from high_c2: 891it [00:02, 320.41it/s]\n",
      "Extracting patches from high_c3: 909it [00:02, 313.86it/s]\n",
      "Extracting patches from high_c1: 108it [00:00, 368.78it/s]\n",
      "Extracting patches from high_c2: 99it [00:00, 390.48it/s]\n",
      "Extracting patches from high_c3: 101it [00:00, 390.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image loaded from path:\n",
      "/localscratch/data/high_c4/high_c4_source.tif\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "\n",
    "data_dir = \"/localscratch/data/\"\n",
    "Three_train_images = ['high_c1', 'high_c2', 'high_c3']\n",
    "\n",
    "# Load source images\n",
    "train_img_paths = [os.path.join(data_dir, img, f\"{img}_source.tif\") for img in Three_train_images]\n",
    "images = {img: tiff.imread(path) for img, path in zip(Three_train_images, train_img_paths)}\n",
    "\n",
    "# Print loaded train images paths\n",
    "print(\"Train images loaded from paths:\")\n",
    "for img, path in zip(Three_train_images, train_img_paths):\n",
    "   print(path)\n",
    "\n",
    "labels = {}\n",
    "# Load ground truth images\n",
    "for img in Three_train_images:\n",
    "   gt_path = os.path.join(data_dir, img, f\"{img}_gt.tif\")\n",
    "   labels[img] = tiff.imread(gt_path)\n",
    "\n",
    "# Initialize dictionaries for the split data\n",
    "train_images = {}\n",
    "val_images = {}\n",
    "train_labels = {}\n",
    "val_labels = {}\n",
    "\n",
    "keys = ['high_c1', 'high_c2', 'high_c3']\n",
    "\n",
    "for key in tqdm(keys, desc='Splitting data'):\n",
    "   filtered_image, filtered_label = boilerplate._filter_slices(images[key], labels[key])\n",
    "   train_image, val_image, train_label, val_label = boilerplate._split_slices(\n",
    "      filtered_image, filtered_label\n",
    "   )\n",
    "   train_images[key] = train_image\n",
    "   val_images[key] = val_image\n",
    "   train_labels[key] = train_label\n",
    "   val_labels[key] = val_label\n",
    "\n",
    "# compute mean and std of the data\n",
    "all_elements = np.concatenate([train_images[key].flatten() for key in keys])\n",
    "data_mean = np.mean(all_elements)\n",
    "data_std = np.std(all_elements)\n",
    "\n",
    "# normalizing the data\n",
    "for key in tqdm(keys, 'Normalizing data'):\n",
    "   train_images[key] = (train_images[key] - data_mean) / data_std\n",
    "   val_images[key] = (val_images[key] - data_mean) / data_std\n",
    "\n",
    "train_set = CustomDataset(train_images, train_labels)\n",
    "val_set = CustomDataset(val_images, val_labels)\n",
    "\n",
    "One_test_image = ['high_c4']\n",
    "\n",
    "# Load test image\n",
    "test_img_path = os.path.join(data_dir, One_test_image[0], f\"{One_test_image[0]}_source.tif\")\n",
    "test_image = tiff.imread(test_img_path)\n",
    "\n",
    "# Print loaded test images paths\n",
    "print(\"Test image loaded from path:\")\n",
    "print(test_img_path)\n",
    "\n",
    "# Load test ground truth images\n",
    "test_gt_path = os.path.join(data_dir, One_test_image[0], f\"{One_test_image[0]}_gt.tif\")\n",
    "test_ground_truth_image = tiff.imread(test_gt_path)\n",
    "\n",
    "bg_indices = random.sample(train_set.patches_by_label[0],sample_size)\n",
    "nucleus_indices = random.sample(train_set.patches_by_label[1],sample_size)\n",
    "granule_indices = random.sample(train_set.patches_by_label[2],sample_size)\n",
    "mito_indices = random.sample(train_set.patches_by_label[3],sample_size)\n",
    "\n",
    "bg_samples, bg_cls, bg_lbl = train_set[bg_indices]\n",
    "nucleus_samples, nucleus_cls, nucleus_lbl = train_set[nucleus_indices]\n",
    "granule_samples, granule_cls, granule_lbl = train_set[granule_indices]\n",
    "mito_samples, mito_cls, mito_lbl = train_set[mito_indices]\n",
    "\n",
    "bg_samples = bg_samples.squeeze(1)\n",
    "bg_lbl = bg_lbl.squeeze(1)\n",
    "nucleus_samples = nucleus_samples.squeeze(1)\n",
    "nucleus_lbl = nucleus_lbl.squeeze(1)\n",
    "granule_samples = granule_samples.squeeze(1)\n",
    "granule_lbl = granule_lbl.squeeze(1)\n",
    "mito_samples = mito_samples.squeeze(1)\n",
    "mito_lbl = mito_lbl.squeeze(1)\n",
    "\n",
    "#test data\n",
    "\n",
    "test_images = (test_image-data_mean)/data_std\n",
    "\n",
    "# test_slice = test_images[626]\n",
    "# test_slice_lbl = test_ground_truth_image[626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = tiff.imread('/group/jug/Sheida/pancreatic beta cells/download/seg/euclidean/high_c4.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test image shape:  (1022, 545, 1082) segmentation shape:  (1022, 482, 1019)\n"
     ]
    }
   ],
   "source": [
    "print('test image shape: ', test_images.shape, 'segmentation shape: ', seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x, y = seg.shape\n",
    "_, v, w = test_ground_truth_image.shape\n",
    "cropped_gt = test_ground_truth_image[:,30:30+x,30:30+y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 482, 1019)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg[test_ground_truth_image[:,30:30+x,30:30+y] == -1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "def compute_ari(gt, pred):\n",
    "    gt_flat = gt.flatten()\n",
    "    pred_flat = pred.flatten()\n",
    "    ari_score = adjusted_rand_score(gt_flat, pred_flat)\n",
    "    return ari_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1022/1022 [00:48<00:00, 21.05it/s]\n"
     ]
    }
   ],
   "source": [
    "ari = 0\n",
    "for i in tqdm(range(seg.shape[0])):\n",
    "    ari += compute_ari(cropped_gt[i], seg[i])\n",
    "ari /= seg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9081470224522128\n"
     ]
    }
   ],
   "source": [
    "print(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1022 [00:00<?, ?it/s]/tmp/ipykernel_2476/3940938958.py:3: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 2 * intersection / (np.sum(y_true == class_label) + np.sum(y_pred == class_label))\n"
     ]
    }
   ],
   "source": [
    "def dice_coefficient_per_class(y_true, y_pred, class_label):\n",
    "    intersection = np.sum((y_true == class_label) & (y_pred == class_label))\n",
    "    return 2 * intersection / (np.sum(y_true == class_label) + np.sum(y_pred == class_label))\n",
    "\n",
    "def dice_coefficient_multilabel(y_true, y_pred, num_classes, ignore_label=-1):\n",
    "    dice_scores = []\n",
    "    for class_label in range(num_classes):\n",
    "        if class_label == ignore_label:\n",
    "            continue\n",
    "        dice = dice_coefficient_per_class(y_true, y_pred, class_label)\n",
    "        dice_scores.append(dice)\n",
    "    return np.array(dice_scores)\n",
    "\n",
    "num_classes = 4  # Number of labels/classes (excluding the -1 label)\n",
    "\n",
    "dsc_per_class = [0,0,0,0]\n",
    "for i in tqdm(range(seg.shape[0])):\n",
    "    dsc_per_class += dice_coefficient_multilabel(cropped_gt[i], seg[i], num_classes)\n",
    "dsc_per_class /= seg.shape[0]\n",
    "print(f\"Train Dice Similarity Coefficient per class: {dsc_per_class}\")\n",
    "print(f\"Train Mean Dice Similarity Coefficient: {np.mean(dsc_per_class):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Filter out the background label (-1)\n",
    "mask = gt_flat != -1\n",
    "y_true_filtered = gt_flat[mask]\n",
    "y_pred_filtered1 = pred_flat[mask]\n",
    "y_pred_filtered2 = clust_flat[mask]\n",
    "\n",
    "# Calculate the Dice coefficient as the F1 score for each class\n",
    "dsc1 = f1_score(y_true_filtered, y_pred_filtered1, average=None)\n",
    "dsc2 = f1_score(y_true_filtered, y_pred_filtered2, average=None)\n",
    "\n",
    "print(f\"Train Dice Similarity Coefficient per class: {dsc1}\")\n",
    "print(f\"Cluster Dice Similarity Coefficient per class: {dsc2}\")\n",
    "\n",
    "print(f\"Train Mean Dice Similarity Coefficient: {np.mean(dsc1):.4f}\")\n",
    "print(f\"Cluster Mean Dice Similarity Coefficient: {np.mean(dsc2):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
