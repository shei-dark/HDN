{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "sys.path.append('/home/sheida.rahnamai/GIT/HDN/')\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tifffile import imread\n",
    "import glob\n",
    "import matplotlib.patches as patches\n",
    "import hdbscan as hd\n",
    "from hdbscan import prediction\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from collections import Counter\n",
    "from openTSNE import TSNE\n",
    "from openTSNE import TSNEEmbedding\n",
    "from openTSNE import affinity\n",
    "from openTSNE import initialization\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "# %reload lib.plotting \n",
    "from lib import plotting as p\n",
    "from lib import dataprep as dp\n",
    "import random\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "from lib.evaluation import FeatureExtractor\n",
    "# from lib.dataloader import CustomTestDataset\n",
    "from lib.dataloader import CustomDataset\n",
    "import tifffile as tiff\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from boilerplate import boilerplate\n",
    "from importlib import reload\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering data: 100%|██████████| 3/3 [00:00<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/group/jug/Sheida/pancreatic beta cells/download/\"\n",
    "keys = ['high_c1', 'high_c2', 'high_c3']\n",
    "\n",
    "# train data\n",
    "train_img_paths = [os.path.join(data_dir, \"train/\", img, f\"{img}_source.tif\") for img in keys]\n",
    "train_lbl_paths = [os.path.join(data_dir, \"train/\", img, f\"{img}_gt.tif\") for img in keys]\n",
    "train_images = {img: tiff.imread(path) for img, path in zip(keys, train_img_paths)}\n",
    "train_labels = {img: tiff.imread(path) for img, path in zip(keys, train_lbl_paths)}\n",
    "#val data\n",
    "val_img_paths = [os.path.join(data_dir, \"val/\", img, f\"{img}_source.tif\") for img in keys]\n",
    "val_lbl_paths = [os.path.join(data_dir, \"val/\", img, f\"{img}_gt.tif\") for img in keys]\n",
    "val_images = {img: tiff.imread(path) for img, path in zip(keys, val_img_paths)}\n",
    "val_labels = {img: tiff.imread(path) for img, path in zip(keys, val_lbl_paths)}\n",
    "\n",
    "\n",
    "for key in tqdm(keys, desc='Filtering data'):\n",
    "   train_images[key], train_labels[key] = boilerplate._filter_slices(train_images[key], train_labels[key])\n",
    "   val_images[key], val_labels[key] = boilerplate._filter_slices(val_images[key], val_labels[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5x5 <br>\n",
    "Uncategorized: 60827 Nucleus: 18054 Granule: 17870 Mitochondria: 7438 <br>\n",
    "Uncategorized: 6980 Nucleus: 1853 Granule: 2109 Mitochondria: 882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5x5 - 3x3 OR 3x3 <br>\n",
    "Uncategorized: 72213 Nucleus: 18462 Granule: 24558 Mitochondria: 9355 <br>\n",
    "Uncategorized: 8314 Nucleus: 1901 Granule: 2885 Mitochondria: 1090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5x5 - 1x1 OR 3x3 - 1x1 OR 1x1<br>\n",
    "Uncategorized: 84693 Nucleus: 18816 Granule: 32293 Mitochondria: 11368 <br>\n",
    "Uncategorized: 9748 Nucleus: 1943 Granule: 3839 Mitochondria: 1313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_percent(file_path, data):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        # Serialize and write the variable to the file\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask size: 5 Label size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 38it [00:00, 379.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 965it [00:02, 370.23it/s]\n",
      "Extracting patches from high_c2: 889it [00:02, 399.69it/s]\n",
      "Extracting patches from high_c3: 907it [00:02, 384.74it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 431.93it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 455.24it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 414.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!!!\n",
      "Train size for uncategorized: 60827, nucleus: 18054, granule: 17870, mitochondria: 7438\n",
      "Val size for uncategorized: 6980, nucleus: 1853, granule: 2109, mitochondria: 882\n",
      "10% of the train set for uncategorized: 6082, nucleus: 1805, granule: 1787, mitochondria: 743\n",
      "1% of the train set for uncategorized: 608, nucleus: 180, granule: 178, mitochondria: 74\n",
      "10% of the val set for uncategorized: 698, nucleus: 185, granule: 210, mitochondria: 88\n",
      "1% of the val set for uncategorized: 69, nucleus: 18, granule: 21, mitochondria: 8\n",
      "Mask size: 5 Label size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 965it [00:02, 335.37it/s]\n",
      "Extracting patches from high_c2: 889it [00:02, 337.02it/s]\n",
      "Extracting patches from high_c3: 907it [00:02, 381.89it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 434.97it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 456.70it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 158.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!!!\n",
      "Train size for uncategorized: 72213, nucleus: 18462, granule: 24558, mitochondria: 9355\n",
      "Val size for uncategorized: 8314, nucleus: 1901, granule: 2885, mitochondria: 1090\n",
      "10% of the train set for uncategorized: 7221, nucleus: 1846, granule: 2455, mitochondria: 935\n",
      "1% of the train set for uncategorized: 722, nucleus: 184, granule: 245, mitochondria: 93\n",
      "10% of the val set for uncategorized: 831, nucleus: 190, granule: 288, mitochondria: 109\n",
      "1% of the val set for uncategorized: 83, nucleus: 19, granule: 28, mitochondria: 10\n",
      "Mask size: 5 Label size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 965it [00:02, 410.22it/s]\n",
      "Extracting patches from high_c2: 889it [00:02, 349.50it/s]\n",
      "Extracting patches from high_c3: 907it [00:03, 274.10it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 410.87it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 442.62it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 391.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!!!\n",
      "Train size for uncategorized: 84693, nucleus: 18816, granule: 32293, mitochondria: 11368\n",
      "Val size for uncategorized: 9748, nucleus: 1943, granule: 3839, mitochondria: 1313\n",
      "10% of the train set for uncategorized: 8469, nucleus: 1881, granule: 3229, mitochondria: 1136\n",
      "1% of the train set for uncategorized: 846, nucleus: 188, granule: 322, mitochondria: 113\n",
      "10% of the val set for uncategorized: 974, nucleus: 194, granule: 383, mitochondria: 131\n",
      "1% of the val set for uncategorized: 97, nucleus: 19, granule: 38, mitochondria: 13\n",
      "Mask size: 3 Label size: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 965it [00:02, 336.40it/s]\n",
      "Extracting patches from high_c2: 889it [00:01, 461.26it/s]\n",
      "Extracting patches from high_c3: 907it [00:02, 307.28it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 430.10it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 460.32it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 413.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!!!\n",
      "Train size for uncategorized: 72213, nucleus: 18462, granule: 24558, mitochondria: 9355\n",
      "Val size for uncategorized: 8314, nucleus: 1901, granule: 2885, mitochondria: 1090\n",
      "10% of the train set for uncategorized: 7221, nucleus: 1846, granule: 2455, mitochondria: 935\n",
      "1% of the train set for uncategorized: 722, nucleus: 184, granule: 245, mitochondria: 93\n",
      "10% of the val set for uncategorized: 831, nucleus: 190, granule: 288, mitochondria: 109\n",
      "1% of the val set for uncategorized: 83, nucleus: 19, granule: 28, mitochondria: 10\n",
      "Mask size: 3 Label size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 965it [00:02, 410.42it/s]\n",
      "Extracting patches from high_c2: 889it [00:02, 331.58it/s]\n",
      "Extracting patches from high_c3: 907it [00:03, 297.04it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 395.07it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 420.84it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 380.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!!!\n",
      "Train size for uncategorized: 84693, nucleus: 18816, granule: 32293, mitochondria: 11368\n",
      "Val size for uncategorized: 9748, nucleus: 1943, granule: 3839, mitochondria: 1313\n",
      "10% of the train set for uncategorized: 8469, nucleus: 1881, granule: 3229, mitochondria: 1136\n",
      "1% of the train set for uncategorized: 846, nucleus: 188, granule: 322, mitochondria: 113\n",
      "10% of the val set for uncategorized: 974, nucleus: 194, granule: 383, mitochondria: 131\n",
      "1% of the val set for uncategorized: 97, nucleus: 19, granule: 38, mitochondria: 13\n",
      "Mask size: 1 Label size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting patches from high_c1: 965it [00:03, 315.16it/s]\n",
      "Extracting patches from high_c2: 889it [00:02, 437.83it/s]\n",
      "Extracting patches from high_c3: 907it [00:03, 273.32it/s]\n",
      "Extracting patches from high_c1: 109it [00:00, 339.08it/s]\n",
      "Extracting patches from high_c2: 101it [00:00, 428.94it/s]\n",
      "Extracting patches from high_c3: 103it [00:00, 380.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created!!!\n",
      "Train size for uncategorized: 84693, nucleus: 18816, granule: 32293, mitochondria: 11368\n",
      "Val size for uncategorized: 9748, nucleus: 1943, granule: 3839, mitochondria: 1313\n",
      "10% of the train set for uncategorized: 8469, nucleus: 1881, granule: 3229, mitochondria: 1136\n",
      "1% of the train set for uncategorized: 846, nucleus: 188, granule: 322, mitochondria: 113\n",
      "10% of the val set for uncategorized: 974, nucleus: 194, granule: 383, mitochondria: 131\n",
      "1% of the val set for uncategorized: 97, nucleus: 19, granule: 38, mitochondria: 13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "patch_size = 64\n",
    "modes = ['5x5', '3x3', '1x1']\n",
    "categories = ['uncategorized', 'nucleus', 'granule', 'mitochondria']\n",
    "\n",
    "for i in range(len(modes)):\n",
    "    mask_size = int(modes[i][0])\n",
    "    label_size = int(modes[i][-1])\n",
    "    print(f\"Mask size: {mask_size} Label size: {label_size}\")\n",
    "    \n",
    "    train_set = CustomDataset(train_images, train_labels, patch_size, mask_size, label_size)\n",
    "    val_set = CustomDataset(val_images, val_labels, patch_size, mask_size, label_size)\n",
    "    print('Datasets created!!!')\n",
    "    \n",
    "    t_uncategorize = train_set.patches_by_label[0]\n",
    "    t_nucleus = train_set.patches_by_label[1]\n",
    "    t_granule = train_set.patches_by_label[2]\n",
    "    t_mitochondria = train_set.patches_by_label[3]\n",
    "    print(f'Train size for {categories[0]}: {len(t_uncategorize)}, {categories[1]}: {len(t_nucleus)}, {categories[2]}: {len(t_granule)}, {categories[3]}: {len(t_mitochondria)}')\n",
    "    \n",
    "    v_uncategorize = val_set.patches_by_label[0]\n",
    "    v_nucleus = val_set.patches_by_label[1]\n",
    "    v_granule = val_set.patches_by_label[2]\n",
    "    v_mitochondria = val_set.patches_by_label[3]\n",
    "    print(f'Val size for {categories[0]}: {len(v_uncategorize)}, {categories[1]}: {len(v_nucleus)}, {categories[2]}: {len(v_granule)}, {categories[3]}: {len(v_mitochondria)}')\n",
    "    \n",
    "    t0 = random.sample(t_uncategorize, len(t_uncategorize)//10)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/10_percent_{categories[0]}.pickle',t0)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/1_percent_{categories[0]}.pickle',random.sample(t0, len(t0)//10))\n",
    "    \n",
    "    t1 = random.sample(t_nucleus, len(t_nucleus)//10)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/10_percent_{categories[1]}.pickle',t1)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/1_percent_{categories[1]}.pickle',random.sample(t1, len(t1)//10))\n",
    "    \n",
    "    t2 = random.sample(t_granule, len(t_granule)//10)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/10_percent_{categories[2]}.pickle',t2)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/1_percent_{categories[2]}.pickle',random.sample(t2, len(t2)//10))\n",
    "    \n",
    "    t3 = random.sample(t_mitochondria, len(t_mitochondria)//10)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/10_percent_{categories[3]}.pickle',t3)\n",
    "    save_percent(data_dir+f'train/{modes[i]}/1_percent_{categories[3]}.pickle',random.sample(t3, len(t3)//10))\n",
    "    \n",
    "    print(f'10% of the train set for {categories[0]}: {len(t0)}, {categories[1]}: {len(t1)}, {categories[2]}: {len(t2)}, {categories[3]}: {len(t3)}')\n",
    "    print(f'1% of the train set for {categories[0]}: {len(t0)//10}, {categories[1]}: {len(t1)//10}, {categories[2]}: {len(t2)//10}, {categories[3]}: {len(t3)//10}')\n",
    "    \n",
    "    v0 = random.sample(v_uncategorize, len(v_uncategorize)//10)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/10_percent_{categories[0]}.pickle',v0)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/1_percent_{categories[0]}.pickle',random.sample(v0, len(v0)//10))\n",
    "    \n",
    "    v1 = random.sample(v_nucleus, len(v_nucleus)//10)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/10_percent_{categories[1]}.pickle',v1)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/1_percent_{categories[1]}.pickle',random.sample(v1, len(v1)//10))\n",
    "    \n",
    "    v2 = random.sample(v_granule, len(v_granule)//10)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/10_percent_{categories[2]}.pickle',v2)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/1_percent_{categories[2]}.pickle',random.sample(v2, len(v2)//10))\n",
    "    \n",
    "    v3 = random.sample(v_mitochondria, len(v_mitochondria)//10)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/10_percent_{categories[3]}.pickle',v3)\n",
    "    save_percent(data_dir+f'val/{modes[i]}/1_percent_{categories[3]}.pickle',random.sample(v3, len(v3)//10))\n",
    "    \n",
    "    print(f'10% of the val set for {categories[0]}: {len(v0)}, {categories[1]}: {len(v1)}, {categories[2]}: {len(v2)}, {categories[3]}: {len(v3)}')\n",
    "    print(f'1% of the val set for {categories[0]}: {len(v0)//10}, {categories[1]}: {len(v1)//10}, {categories[2]}: {len(v2)//10}, {categories[3]}: {len(v3)//10}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
