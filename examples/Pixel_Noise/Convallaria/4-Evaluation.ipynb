{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions import kl_divergence\n",
    "from tifffile import imread\n",
    "from tqdm import tqdm\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "patch_size = 64\n",
    "centre_size = 4\n",
    "n_channel = 32\n",
    "hierarchy_level = 5\n",
    "limit = 400\n",
    "first_model = ' base'\n",
    "second_model = ' test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(dir):\n",
    "    model = torch.load(dir+\"Contrastive_MAE_best_vae.net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_losses(dir):\n",
    "    trainHist=np.load(dir+\"train_loss.npy\")\n",
    "    reconHist=np.load(dir+\"train_reco_loss.npy\")\n",
    "    klHist=np.load(dir+\"train_kl_loss.npy\")\n",
    "    clHist=np.load(dir+\"train_cl_loss.npy\")\n",
    "    valHist=np.load(dir+\"val_loss.npy\")\n",
    "    return [trainHist, reconHist, klHist, clHist, valHist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(losses['trainHist'],label='training')\n",
    "    plt.plot(losses['valHist'],label='validation')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"overall loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(losses['reconHist'],label='training')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Inpainting loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(losses['klHist'],label='training')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"KL loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(losses['clHist'],label='training')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"CL loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_loss(base_loss, cl_loss):\n",
    "    \n",
    "    plt.figure(figsize=(18, 18))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(base_loss['trainHist'],label='training'+first_model)\n",
    "    plt.plot(base_loss['valHist'],label='validation'+first_model)\n",
    "\n",
    "    plt.plot(cl_loss['trainHist'],label='training'+second_model)\n",
    "    plt.plot(cl_loss['valHist'],label='validation'+second_model)\n",
    "\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"overall loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(base_loss['reconHist'],label='training'+first_model)\n",
    "\n",
    "    plt.plot(cl_loss['reconHist'],label='training'+second_model)\n",
    "\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Inpainting loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(base_loss['klHist'],label='training'+first_model)\n",
    "\n",
    "    plt.plot(cl_loss['klHist'],label='training'+second_model)\n",
    "\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"KL loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(base_loss['clHist'],label='training'+first_model)\n",
    "\n",
    "    plt.plot(cl_loss['clHist'],label='training'+second_model)\n",
    "\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"CL loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_tensor(img,model,device):\n",
    "    test_images = torch.from_numpy(img.copy()).to(device)\n",
    "    data_mean = model.data_mean\n",
    "    data_std = model.data_std\n",
    "    test_images = (test_images-data_mean)/data_std\n",
    "    return test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(z1, z2):\n",
    "    z1 = torch.from_numpy(z1)\n",
    "    z2 = torch.from_numpy(z2)\n",
    "    return torch.mean(F.cosine_similarity(z1, z2, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_dis(z1, z2):\n",
    "    return np.mean(np.sqrt(((z1-z2)**2).sum(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_d(z1, z2):\n",
    "    m1, logv1 = z1\n",
    "    m2, logv2 = z2\n",
    "    m1 = torch.from_numpy(m1)\n",
    "    logv1 = torch.from_numpy(logv1)\n",
    "    m2 = torch.from_numpy(m2)\n",
    "    logv2 = torch.from_numpy(logv2)\n",
    "    std1 = (logv1 / 2).exp()\n",
    "    std2 = (logv2 / 2).exp()\n",
    "    dis1 = Normal(m1, std1)\n",
    "    dis2 = Normal(m2, std2)\n",
    "    temp = kl_divergence(dis1, dis2)\n",
    "    temp = torch.reshape(temp, (-1, 32))\n",
    "    return torch.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centre(x, i):\n",
    "    if i == 3:\n",
    "        return x[i][0].cpu().numpy()\n",
    "    elif i == 4:\n",
    "        return x[i][0].cpu().numpy().repeat(2,axis=1).repeat(2,axis=2)\n",
    "    else:\n",
    "        lower_bound = 2**(hierarchy_level-1-i)-int(centre_size/2)\n",
    "        upper_bound = 2**(hierarchy_level-1-i)+int(centre_size/2)\n",
    "        return x[i][0].cpu().numpy()[:,lower_bound:upper_bound,lower_bound:upper_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(model, z1, z2, mode):\n",
    "    n_features = n_channel * hierarchy_level\n",
    "    data1 = np.zeros((n_features, centre_size, centre_size))\n",
    "    data2 = np.zeros((n_features, centre_size, centre_size))\n",
    "    model.mode_pred=True\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    z1 = z1.to(device=device, dtype=torch.float)\n",
    "    z1 = z1.reshape(1,1,patch_size,patch_size)\n",
    "    z2 = z2.to(device=device, dtype=torch.float)\n",
    "    z2 = z2.reshape(1,1,patch_size,patch_size)\n",
    "    with torch.no_grad():\n",
    "            sample1 = model(z1, z1)\n",
    "            sample2 = model(z2, z2)\n",
    "            mu1 = sample1['mu']\n",
    "            mu2 = sample2['mu']\n",
    "            for i in range(hierarchy_level):\n",
    "                data1[i*n_channel:(i+1)*n_channel] = get_centre(mu1, i)\n",
    "                data2[i*n_channel:(i+1)*n_channel] = get_centre(mu2, i)\n",
    "            data1 = data1.T.reshape(-1,n_features)\n",
    "            data2 = data2.T.reshape(-1,n_features)\n",
    "\n",
    "            if mode == 'euclidean distance':\n",
    "                return euc_dis(data1, data2)\n",
    "            elif mode == 'cosine similarity':\n",
    "                return cos_similarity(data1, data2)\n",
    "            elif mode == 'kl divergence':\n",
    "                data3 = np.zeros((n_features, centre_size, centre_size))\n",
    "                data4 = np.zeros((n_features, centre_size, centre_size))\n",
    "                logvar1 = sample1['logvar']\n",
    "                logvar2 = sample2['logvar']\n",
    "                for i in range(hierarchy_level):\n",
    "                    data3[i*n_channel:(i+1)*n_channel] = get_centre(logvar1, i)\n",
    "                    data4[i*n_channel:(i+1)*n_channel] = get_centre(logvar2, i)\n",
    "                data3 = data3.T.reshape(-1,n_features)\n",
    "                data4 = data4.T.reshape(-1,n_features)\n",
    "                return kl_d([data1, data3], [data2,data4])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_hist(hist, label):\n",
    "    n_bins = 100\n",
    "    _, axs = plt.subplots(1, 1, sharey=False, tight_layout=False)\n",
    "    plt.title(label)\n",
    "    axs.hist(hist, bins=n_bins)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_double_hist(hist, label, title):\n",
    "    plt.hist(hist, bins=100, label=label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triple_hist(hist, label, title):\n",
    "    plt.hist(hist, bins=20, label=label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_hist(data, model, mode):\n",
    "    hist = []\n",
    "    for i in tqdm(range(int(len(data)/2))):\n",
    "        hist.append(get_distance(model,data[i],data[len(data)-i-1],mode))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neg_hist(class1, class2, limit, model, mode):\n",
    "    hist = []\n",
    "    for i in tqdm(range(limit)):\n",
    "        hist.append(get_distance(model, class1[i], class2[i], mode))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir):\n",
    "    return imread(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/group/jug/Sheida/HDN_models/27022024/Contrastive_False/model/\"\n",
    "cl_dir = \"/home/sheida.rahnamai/GIT/HDN/examples/Pixel_Noise/Convallaria/test/model/\"\n",
    "data_dir = \"/group/jug/Sheida/pancreatic beta cells/download/high_c1/contrastive/patches/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = load_model(base_dir)\n",
    "model_cl = load_model(cl_dir)\n",
    "loss_keys = ['trainHist', 'reconHist', 'klHist', 'clHist', 'valHist']\n",
    "base_loss_dict = dict()\n",
    "cl_loss_dict = dict()\n",
    "base_losses = load_losses(base_dir)\n",
    "cl_losses = load_losses(cl_dir)\n",
    "for index, key in enumerate(loss_keys):\n",
    "    base_loss_dict[key] = base_losses[index]\n",
    "    cl_loss_dict[key] = cl_losses[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(base_loss_dict)\n",
    "plot_loss(cl_loss_dict)\n",
    "plot_double_loss(base_loss_dict, cl_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golgi = get_normalized_tensor(load_data(data_dir+'golgi/*.tif'), model_base, device)\n",
    "mitochondria = get_normalized_tensor(load_data(data_dir+'mitochondria/*.tif'), model_base, device)\n",
    "granule = get_normalized_tensor(load_data(data_dir+'granules/*.tif'), model_base, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = ['euclidean distance', 'cosine similarity', 'kl divergence']\n",
    "class_type = [golgi, mitochondria, granule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_name(var):\n",
    "    for name, value in globals().items():\n",
    "        if value is var:\n",
    "            return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dis_type in mode:\n",
    "    for i in range(len(class_type)):\n",
    "        cl_type = get_var_name(class_type[i])\n",
    "        x = compute_pos_hist(class_type[i], model_base, dis_type)\n",
    "        y = compute_pos_hist(class_type[i], model_cl, dis_type)\n",
    "        plot_double_hist([x,y], [first_model, second_model], cl_type+' '+dis_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dis_type in mode:\n",
    "    for i in range(len(class_type)):\n",
    "        for j in range(i+1,len(class_type)):\n",
    "            cl_type = get_var_name(class_type[i])\n",
    "            opp_type = get_var_name(class_type[j])\n",
    "            x = compute_neg_hist(class_type[i], class_type[j], limit, model_base, dis_type)\n",
    "            y = compute_neg_hist(class_type[i], class_type[j], limit, model_cl, dis_type)\n",
    "            plot_double_hist([x,y], [first_model, second_model], cl_type+' '+opp_type+' '+dis_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triple_hist(hist, label, title):\n",
    "    n_bins = 20\n",
    "    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "    fig.set_figwidth(18)\n",
    "    fig.set_figheight(9)\n",
    "    axs[0].hist(hist[0], bins=n_bins, label=label)\n",
    "    axs[0].set_title(title[0])\n",
    "    axs[0].legend()\n",
    "    axs[1].hist(hist[1], bins=n_bins, label=label)\n",
    "    axs[1].set_title(title[1])\n",
    "    axs[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dis_type in mode:\n",
    "    for i in range(len(class_type)):\n",
    "        j = (i+1)%len(class_type)\n",
    "        k = (i+2)%len(class_type)\n",
    "        cl_type = get_var_name(class_type[i])\n",
    "        opp_1 = get_var_name(class_type[j])\n",
    "        opp_2 = get_var_name(class_type[k])\n",
    "        x = compute_pos_hist(class_type[i], model_base, dis_type)\n",
    "        y = compute_neg_hist(class_type[i], class_type[j], limit, model_base, dis_type)\n",
    "        z = compute_neg_hist(class_type[i], class_type[k], limit, model_base, dis_type)\n",
    "        a = compute_pos_hist(class_type[i], model_cl, dis_type)\n",
    "        b = compute_neg_hist(class_type[i], class_type[j], limit, model_cl, dis_type)\n",
    "        c = compute_neg_hist(class_type[i], class_type[k], limit, model_cl, dis_type)\n",
    "        plot_triple_hist([[x,y,z],\n",
    "                        [a,b,c]],\n",
    "                        ['+ '+cl_type, '- '+opp_1, '- '+opp_2],\n",
    "                        [cl_type+' '+dis_type+first_model,\n",
    "                        cl_type+' '+dis_type+second_model])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
